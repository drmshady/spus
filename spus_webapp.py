# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175PonR7Nc8EVaVO7AdmE3Fnu5S9OSq7r
"""

# -*- coding: utf-8 -*-
"""
SPUS Stock Analyzer Web Application
Powered by Streamlit
"""

# --- Core Imports ---
import streamlit as st
import requests
import yfinance as yf
import pandas as pd
import pandas_ta as ta
import time
import os
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import openpyxl
from openpyxl.styles import Font
import io
import json
import requests.exceptions

# --- PDF Report Imports ---
try:
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import landscape, letter
    from reportlab.lib.units import inch
    REPORTLAB_INSTALLED = True
except ImportError:
    REPORTLAB_INSTALLED = False
    # We'll handle this error in the UI

# ----------------------------------------------------------------------
# --- HELPER FUNCTIONS (Copied from spus_v2.py) ---
# ----------------------------------------------------------------------

def load_config(path='config.json'):
    """Loads the external config.json file."""
    try:
        with open(path, 'r') as f:
            config = json.load(f)
        logging.info(f"Successfully loaded configuration from {path}")
        return config
    except FileNotFoundError:
        logging.error(f"FATAL: Configuration file '{path}' not found.")
        return None
    except json.JSONDecodeError:
        logging.error(f"FATAL: Could not decode JSON from '{path}'. Check for syntax errors.")
        return None

def fetch_spus_tickers(uploaded_file_object):
    """
    Fetches SPUS holdings tickers by reading an uploaded file object.
    MODIFIED for Streamlit to accept a file object instead of a path.
    """
    ticker_column_name = 'StockTicker' # Correct column name

    if uploaded_file_object is None:
        logging.error("No file object provided to fetch_spus_tickers.")
        return []

    try:
        # Read the uploaded file object directly
        holdings_df = pd.read_csv(uploaded_file_object)

    except pd.errors.ParserError:
        logging.warning("Pandas ParserError. Trying again with 'skiprows'...")
        # Reset file pointer and try again
        uploaded_file_object.seek(0)
        for i in range(1, 10):
            try:
                holdings_df = pd.read_csv(uploaded_file_object, skiprows=i)
                uploaded_file_object.seek(0) # Reset pointer
                if ticker_column_name in holdings_df.columns:
                    logging.info(f"Successfully parsed CSV by skipping {i} rows.")
                    break
            except Exception:
                uploaded_file_object.seek(0) # Reset pointer
                continue
        else:
            logging.error(f"Failed to parse uploaded CSV even after skipping 9 rows.")
            return []
    except Exception as e:
        logging.error(f"An unexpected error occurred during CSV read/parse: {e}")
        return []

    if ticker_column_name not in holdings_df.columns:
        logging.error(f"Uploaded CSV, but '{ticker_column_name}' column not found.")
        return []

    try:
        holdings_df[ticker_column_name] = holdings_df[ticker_column_name].astype(str)
        first_nan_index = holdings_df[holdings_df[ticker_column_name].isna()].index[0]
        holdings_df = holdings_df.iloc[:first_nan_index]
        logging.info("Removed footer metadata from CSV.")
    except IndexError:
        pass # No NaN rows found
    except Exception as e:
         logging.warning(f"Error cleaning footer metadata: {e}")
         pass

    ticker_symbols = holdings_df[ticker_column_name].tolist()
    ticker_symbols = [s for s in ticker_symbols if isinstance(s, str) and s and s != ticker_column_name and 'CASH' not in s]
    logging.info(f"Successfully fetched {len(ticker_symbols)} ticker symbols from uploaded file.")
    return ticker_symbols

def calculate_support_resistance(hist_df):
    if hist_df is None or hist_df.empty:
        return None, None
    support = hist_df['Low'].min()
    resistance = hist_df['High'].max()
    return support, resistance

def calculate_financials_and_fair_price(ticker_obj, last_price):
    try:
        info = ticker_obj.info
        pe_ratio = info.get('forwardPE', None)
        pb_ratio = info.get('priceToBook', None)
        div_yield = info.get('dividendYield', None)
        debt_to_equity = info.get('debtToEquity', None)
        rev_growth = info.get('revenueGrowth', None)
        roe = info.get('returnOnEquity', None)
        ev_ebitda = info.get('enterpriseToEbitda', None)
        ps_ratio = info.get('priceToSalesTrailing12Months', None)
        high_52wk = info.get('fiftyTwoWeekHigh', None)
        low_52wk = info.get('fiftyTwoWeekLow', None)
        eps = info.get('trailingEps', None)
        bvps = None
        graham_number = None
        valuation_signal = "N/A"

        if eps is not None and pb_ratio is not None and eps > 0 and pb_ratio > 0 and last_price is not None:
            bvps = last_price / pb_ratio
            graham_number = (22.5 * eps * bvps) ** 0.5
            if last_price < graham_number:
                valuation_signal = "Undervalued (Graham)"
            else:
                valuation_signal = "Overvalued (Graham)"
        elif eps is not None and eps <= 0:
            valuation_signal = "Unprofitable (EPS < 0)"

        financial_dict = {
            'Forward P/E': pe_ratio, 'P/B Ratio': pb_ratio,
            'Dividend Yield': div_yield * 100 if div_yield else None,
            'Debt/Equity': debt_to_equity,
            'Revenue Growth (QoQ)': rev_growth * 100 if rev_growth else None,
            'Return on Equity (ROE)': roe * 100 if roe else None,
            'EV/EBITDA': ev_ebitda, 'Price/Sales (P/S)': ps_ratio,
            '52 Week High': high_52wk, '52 Week Low': low_52wk,
            'Graham Number': graham_number, 'Valuation (Graham)': valuation_signal
        }
        return financial_dict
    except Exception as e:
        logging.error(f"Error fetching fundamental data for {ticker_obj.ticker}: {e}")
        return {key: None for key in ['Forward P/E', 'P/B Ratio', 'Dividend Yield', 'Debt/Equity', 'Revenue Growth (QoQ)', 'Graham Number', 'Valuation (Graham)', 'Return on Equity (ROE)', 'EV/EBITDA', 'Price/Sales (P/S)', '52 Week High', '52 Week Low']}

def calculate_quant_score(data):
    score = 0
    try:
        if data.get('Valuation (Graham)') == 'Undervalued (Graham)': score += 3
        if data.get('MACD_Signal') == 'Bullish Crossover (Favorable)': score += 2
        elif data.get('MACD_Signal') == 'Bullish (Favorable)': score += 1
        if data.get('Trend (50/200 Day MA)') == 'Uptrend': score += 2
        if data.get('Price vs. Levels') == 'Near Support': score += 1
        rsi_val = data.get('RSI (14-day)')
        if rsi_val is not None and not pd.isna(rsi_val):
            if rsi_val < 40: score += 1
            elif rsi_val > 70: score -= 1
        rev_growth_val = data.get('Revenue Growth (QoQ)')
        if rev_growth_val is not None and not pd.isna(rev_growth_val) and rev_growth_val > 0: score += 1
        debt_equity_val = data.get('Debt/Equity')
        if debt_equity_val is not None and not pd.isna(debt_equity_val) and debt_equity_val < 100: score += 1
    except Exception as e:
        logging.warning(f"Could not calculate quant score for {data.get('Ticker', 'N/A')}: {e}")
        return None
    return score

def process_ticker(ticker, CONFIG):
    """Fetches, processes, and analyzes data for a single ticker."""
    null_return = {'ticker': ticker, 'success': False}
    file_path = os.path.join(CONFIG['HISTORICAL_DATA_DIR'], f'{ticker}.csv')
    existing_hist_df = pd.DataFrame()
    start_date = None

    if os.path.exists(file_path):
        try:
            existing_hist_df = pd.read_csv(file_path, index_col='Date', parse_dates=True)
            if not existing_hist_df.empty:
                existing_hist_df.index = pd.to_datetime(existing_hist_df.index, utc=True)
                last_date = existing_hist_df.index.max()
                start_date = last_date + timedelta(days=1)
        except Exception as e:
            logging.warning(f"Error loading existing data for {ticker}: {e}")
            existing_hist_df = pd.DataFrame()

    new_hist = pd.DataFrame()
    ticker_obj = None
    fetch_success = False
    for attempt in range(3):
        try:
            ticker_obj = yf.Ticker(ticker)
            if start_date:
                new_hist = ticker_obj.history(start=start_date.strftime('%Y-%m-%d'))
            else:
                new_hist = ticker_obj.history(period=CONFIG['HISTORICAL_DATA_PERIOD'])
            fetch_success = True
            break
        except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout, TimeoutError) as e:
            logging.warning(f"[{ticker}] Network error on attempt {attempt+1}/3: {e}. Retrying in {5*(attempt+1)}s...")
            time.sleep(5 * (attempt+1))
        except Exception as e:
            logging.error(f"Error fetching new history for {ticker}: {e}")
            break

    if not fetch_success and existing_hist_df.empty:
        logging.error(f"[{ticker}] Failed to fetch history after 3 attempts.")
        return null_return

    if not new_hist.empty or not existing_hist_df.empty:
        combined_hist = pd.concat([existing_hist_df, new_hist]).drop_duplicates(keep='last').sort_index()
        if combined_hist.empty: return null_return
        if not combined_hist.index.is_unique:
             combined_hist = combined_hist[~combined_hist.index.duplicated(keep='first')]

        hist = combined_hist
        try:
            hist.ta.rsi(length=CONFIG['RSI_WINDOW'], append=True)
            hist.ta.sma(length=CONFIG['SHORT_MA_WINDOW'], append=True)
            hist.ta.sma(length=CONFIG['LONG_MA_WINDOW'], append=True)
            hist.ta.macd(fast=CONFIG['MACD_SHORT_SPAN'], slow=CONFIG['MACD_LONG_SPAN'], signal=CONFIG['MACD_SIGNAL_SPAN'], append=True)
        except Exception as e:
            logging.warning(f"Error calculating TA for {ticker}: {e}. Skipping TA.")
            pass

        rsi_col = f'RSI_{CONFIG["RSI_WINDOW"]}'
        short_ma_col = f'SMA_{CONFIG["SHORT_MA_WINDOW"]}'
        long_ma_col = f'SMA_{CONFIG["LONG_MA_WINDOW"]}'
        macd_col = f'MACD_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'
        macd_h_col = f'MACDh_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'
        macd_s_col = f'MACDs_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'

        if not hist.empty:
            last_price = hist['Close'].iloc[-1]
            momentum = None
            if len(hist) >= CONFIG['MIN_DATA_POINTS_MOMENTUM']:
                try:
                    price_now = hist['Close'].iloc[-1]
                    price_year_ago = hist['Close'].iloc[-CONFIG['MIN_DATA_POINTS_MOMENTUM']]
                    momentum = ((price_now - price_year_ago) / price_year_ago) * 100
                except IndexError: pass

            rsi = hist[rsi_col].iloc[-1] if rsi_col in hist.columns else None
            last_short_ma = hist[short_ma_col].iloc[-1] if short_ma_col in hist.columns else None
            last_long_ma = hist[long_ma_col].iloc[-1] if long_ma_col in hist.columns else None
            macd = hist[macd_col].iloc[-1] if macd_col in hist.columns else None
            hist_val = hist[macd_h_col].iloc[-1] if macd_h_col in hist.columns else None
            signal_line = hist[macd_s_col].iloc[-1] if macd_s_col in hist.columns else None

            trend = 'Insufficient data for trend'
            if not pd.isna(last_short_ma) and not pd.isna(last_long_ma):
                if last_short_ma > last_long_ma: trend = 'Uptrend'
                elif last_short_ma < last_long_ma: trend = 'Downtrend'
                else: trend = 'No clear trend'

            macd_signal = "N/A"
            if macd_h_col in hist.columns and len(hist) >= 2 and not pd.isna(hist_val):
                prev_hist = hist[macd_h_col].iloc[-2]
                if not pd.isna(prev_hist):
                    if hist_val > 0 and prev_hist <= 0: macd_signal = "Bullish Crossover (Favorable)"
                    elif hist_val < 0 and prev_hist >= 0: macd_signal = "Bearish Crossover (Unfavorable)"
                    elif hist_val > 0: macd_signal = "Bullish (Favorable)"
                    elif hist_val < 0: macd_signal = "Bearish (Unfavorable)"

            support, resistance = calculate_support_resistance(hist)
            support_resistance = {'Support': support, 'Resistance': resistance} if support is not None else None
            financial_dict = calculate_financials_and_fair_price(ticker_obj, last_price)
            combined_hist.to_csv(file_path)

            return {
                'ticker': ticker, 'momentum': momentum, 'rsi': rsi, 'last_price': last_price,
                'support_resistance': support_resistance, 'trend': trend, 'macd': macd,
                'signal_line': signal_line, 'hist_val': hist_val, 'macd_signal': macd_signal,
                'financial_dict': financial_dict, 'success': True
            }
    return null_return

def create_pdf_table(title, df, styles):
    """Helper function to create a PDF table element."""
    if df.empty:
        return [Paragraph(f"No data for: {title}", styles['h2']), Spacer(1, 0.1*inch)]

    df_reset = df.reset_index()
    cols = [df_reset.columns[0]] + [col for col in df.columns if col in df_reset.columns]

    if title == 'Top 10 by Quant Score':
        df_pdf = df_reset[cols[:7]]
    elif title == 'Top 10 Undervalued (Graham)':
        df_pdf = df_reset[['Ticker', 'Quant Score', 'Last Price', 'Fair Price (Graham)']]
    elif title == 'New Bullish Crossovers (MACD)':
         df_pdf = df_reset[['Ticker', 'Quant Score', 'MACD_Signal', 'Last Price', 'Trend (50/200 Day MA)']]
    elif title == 'Stocks Currently Near Support':
         df_pdf = df_reset[['Ticker', 'Quant Score', 'Price vs. Levels', 'Last Price', '% to Support']]
    else:
        df_pdf = df_reset

    data = [df_pdf.columns.tolist()] + df_pdf.values.tolist()
    formatted_data = [data[0]]
    for row in data[1:]:
        new_row = [f"{item:.2f}" if isinstance(item, (float, int)) else str(item) for item in row]
        formatted_data.append(new_row)

    table_style = TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.green),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke), # <-- Corrected typo
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        ('TEXTCOLOR', (0, 1), (-1, -1), colors.black),
        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 1), (-1, -1), 9),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
         ('ALTERNATINGBACKGROUND', (0, 1), (-1, -1), [colors.Color(0.9, 0.9, 0.9), colors.Color(0.98, 0.98, 0.98)])
    ])

    table = Table(formatted_data, hAlign='LEFT')
    table.setStyle(table_style)
    return [Paragraph(title, styles['h2']), Spacer(1, 0.1*inch), table, Spacer(1, 0.25*inch)]

# ----------------------------------------------------------------------
# --- MAIN ANALYSIS FUNCTION (Refactored from __main__) ---
# ----------------------------------------------------------------------

def run_analysis(CONFIG, uploaded_file):
    """
    Runs the entire analysis process.
    This function contains the logic from the original `if __name__ == "__main__"` block.
    """

    # Setup logging (Streamlit can show this in the console)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(CONFIG['LOG_FILE_PATH']),
            logging.StreamHandler()
        ]
    )

    logging.info("Starting SPUS Ticker Fetch...")
    # Use the modified fetch function
    ticker_symbols = fetch_spus_tickers(uploaded_file)
    if not ticker_symbols:
        logging.warning("No ticker symbols fetched. Exiting.")
        st.error("Could not read ticker symbols from the uploaded CSV.")
        return None, None, None

    exclude_tickers = CONFIG['EXCLUDE_TICKERS']
    ticker_symbols = [ticker for ticker in ticker_symbols if ticker not in exclude_tickers]

    if CONFIG['TICKER_LIMIT'] > 0:
        ticker_symbols = ticker_symbols[:CONFIG['TICKER_LIMIT']]
        logging.info(f"Limiting analysis to {CONFIG['TICKER_LIMIT']} tickers.")

    historical_data_dir = CONFIG['HISTORICAL_DATA_DIR']
    if not os.path.exists(historical_data_dir):
        os.makedirs(historical_data_dir)
        logging.info(f"Created '{historical_data_dir}' directory.")

    momentum_data, rsi_data, last_prices, support_resistance_levels = {}, {}, {}, {}
    trend_data, macd_data, financial_data = {}, {}, {}
    processed_tickers = set()
    MAX_WORKERS = CONFIG['MAX_CONCURRENT_WORKERS']

    logging.info(f"Starting concurrent data fetch with {MAX_WORKERS} workers...")
    start_time = time.time()

    # Streamlit progress bar
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_tickers = len(ticker_symbols)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Pass CONFIG to process_ticker
        future_to_ticker = {executor.submit(process_ticker, ticker, CONFIG): ticker for ticker in ticker_symbols}

        for i, future in enumerate(as_completed(future_to_ticker)):
            ticker = future_to_ticker[future]
            try:
                result = future.result(timeout=60)
                if result['success']:
                    processed_tickers.add(ticker)
                    if result.get('momentum') is not None: momentum_data[ticker] = result['momentum']
                    if result.get('rsi') is not None: rsi_data[ticker] = result['rsi']
                    if result.get('last_price') is not None: last_prices[ticker] = result['last_price']
                    if result.get('support_resistance') is not None: support_resistance_levels[ticker] = result['support_resistance']
                    trend_data[ticker] = result['trend']
                    macd_data[ticker] = {
                        'MACD': result.get('macd'), 'Signal_Line': result.get('signal_line'),
                        'Histogram': result.get('hist_val'), 'Signal': result.get('macd_signal')
                    }
                    financial_data[ticker] = result['financial_dict']

                # Update Streamlit UI
                progress = (i + 1) / total_tickers
                progress_bar.progress(progress)
                status_text.text(f"Processing {i+1}/{total_tickers}: {ticker}")

            except TimeoutError:
                 logging.error(f"Processing {ticker} timed out.")
            except Exception as e:
                logging.error(f"Error processing {ticker} in main loop: {e}")

    end_time = time.time()
    logging.info(f"Finished processing. Total time: {end_time - start_time:.2f}s.")
    status_text.text(f"Processed {len(processed_tickers)} tickers. Aggregating results...")

    # --- 4. Compare last price to support/resistance levels ---
    threshold_percentage = CONFIG['PRICE_THRESHOLD_PERCENT']
    comparison_results, support_diff_percentages, resistance_diff_percentages = {}, {}, {}
    for ticker in last_prices.keys():
        last_price = last_prices.get(ticker)
        levels = support_resistance_levels.get(ticker)
        if last_price is not None and levels is not None:
            support = levels.get('Support')
            resistance = levels.get('Resistance')
            if support is not None and resistance is not None:
                support_diff_percentage = ((last_price - support) / support) * 100 if support != 0 else float('inf')
                resistance_diff_percentage = ((last_price - resistance) / resistance) * 100 if resistance != 0 else float('inf')
                support_diff_percentages[ticker] = support_diff_percentage
                resistance_diff_percentages[ticker] = resistance_diff_percentage

                if abs(support_diff_percentage) <= threshold_percentage: comparison_results[ticker] = 'Near Support'
                elif abs(resistance_diff_percentage) <= threshold_percentage: comparison_results[ticker] = 'Near Resistance'
                elif last_price > resistance: comparison_results[ticker] = 'Above Resistance'
                elif last_price < support: comparison_results[ticker] = 'Below Support'
                else: comparison_results[ticker] = 'Between Support and Resistance'
            else:
                comparison_results[ticker] = 'S/R levels not available'
        else:
            comparison_results[ticker] = 'Price or S/R levels not available'

    # --- 5. Aggregate and save results ---
    logging.info("Aggregating, Scoring, and Building Report...")
    tickers_to_report = list(momentum_data.keys())
    if not tickers_to_report:
        logging.warning("No tickers had sufficient data.")
        st.warning("No tickers had sufficient data for a full report.")
        return None, None, None

    results_list = []
    for ticker in tickers_to_report:
        momentum = momentum_data.get(ticker, "N/A")
        rsi = rsi_data.get(ticker, "N/A")
        last_price = last_prices.get(ticker, "N/A")
        support_resistance = support_resistance_levels.get(ticker, {"Support": "N/A", "Resistance": "N/A"})
        trend = trend_data.get(ticker, "N/A")
        price_vs_levels = comparison_results.get(ticker, "N/A")
        pct_to_support = support_diff_percentages.get(ticker, "N/A")
        pct_to_resistance = resistance_diff_percentages.get(ticker, "N/A")
        macd_info = macd_data.get(ticker, {})
        fin_info = financial_data.get(ticker, {})

        try: momentum_str = f"{momentum:.2f}"
        except: momentum_str = "N/A"
        try: pct_to_support_str = f"{pct_to_support:.2f}"
        except: pct_to_support_str = "N/A"
        try: pct_to_resistance_str = f"{pct_to_resistance:.2f}"
        except: pct_to_resistance_str = "N/A"
        try: div_yield_str = f"{fin_info.get('Dividend Yield', 'N/A'):.2f}"
        except: div_yield_str = "N/A"
        try: rev_growth_str = f"{fin_info.get('Revenue Growth (QoQ)', 'N/A'):.2f}"
        except: rev_growth_str = "N/A"

        result_data = {
            'Ticker': ticker, 'Valuation (Graham)': fin_info.get('Valuation (Graham)'),
            'MACD_Signal': macd_info.get('Signal'), 'Trend (50/200 Day MA)': trend,
            'Price vs. Levels': price_vs_levels, 'Last Price': last_price,
            'Fair Price (Graham)': fin_info.get('Graham Number'), 'Forward P/E': fin_info.get('Forward P/E'),
            'P/B Ratio': fin_info.get('P/B Ratio'), 'Dividend Yield (%)': div_yield_str,
            'Debt/Equity': fin_info.get('Debt/Equity'), 'Revenue Growth (QoQ)': rev_growth_str,
            '1-Year Momentum (%)': momentum_str, 'RSI (14-day)': rsi,
            'Support': support_resistance['Support'], 'Resistance': support_resistance['Resistance'],
            '% to Support': pct_to_support_str, '% to Resistance': pct_to_resistance_str,
            'Return on Equity (ROE)': fin_info.get('Return on Equity (ROE)'),
            'EV/EBITDA': fin_info.get('EV/EBITDA'), 'Price/Sales (P/S)': fin_info.get('Price/Sales (P/S)'),
            '52 Week High': fin_info.get('52 Week High'), '52 Week Low': fin_info.get('52 Week Low')
        }

        score_input = {
            'Valuation (Graham)': fin_info.get('Valuation (Graham)'), 'MACD_Signal': macd_info.get('Signal'),
            'Trend (50/200 Day MA)': trend, 'Price vs. Levels': price_vs_levels,
            'RSI (14-day)': rsi_data.get(ticker, None),
            'Revenue Growth (QoQ)': fin_info.get('Revenue Growth (QoQ)'),
            'Debt/Equity': fin_info.get('Debt/Equity')
        }
        result_data['Quant Score'] = calculate_quant_score(score_input)
        results_list.append(result_data)

    columns_order = [
        'Quant Score', 'Ticker', 'Valuation (Graham)', 'MACD_Signal', 'Trend (50/200 Day MA)',
        'Price vs. Levels', 'Last Price', 'Fair Price (Graham)', 'Forward P/E', 'P/B Ratio',
        'Dividend Yield (%)', 'Debt/Equity', 'Revenue Growth (QoQ)',
        '1-Year Momentum (%)', 'RSI (14-day)', 'Support', 'Resistance', '% to Support', '% to Resistance',
        'Return on Equity (ROE)', 'EV/EBITDA', 'Price/Sales (P/S)', '52 Week High', '52 Week Low'
    ]
    results_df = pd.DataFrame(results_list, columns=columns_order)
    results_df.sort_values(by='Quant Score', ascending=False, inplace=True)
    results_df.set_index('Ticker', inplace=True)

    top_10_quant = results_df.head(10)
    top_10_undervalued = results_df[results_df['Valuation (Graham)'] == 'Undervalued (Graham)'].sort_values(by='Quant Score', ascending=False).head(10)
    new_crossovers = results_df[results_df['MACD_Signal'] == 'Bullish Crossover (Favorable)'].sort_values(by='Quant Score', ascending=False).head(10)
    near_support = results_df[results_df['Price vs. Levels'] == 'Near Support'].sort_values(by='Quant Score', ascending=False).head(10)

    # --- Save to Excel ---
    excel_file_path = CONFIG['EXCEL_FILE_PATH']
    try:
        with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:
            results_df.to_excel(writer, sheet_name='All Results', index=True)
            top_10_quant.to_excel(writer, sheet_name='Top 10 Quant Score', index=True)
            top_10_undervalued.to_excel(writer, sheet_name='Top 10 Undervalued (Graham)', index=True)
            new_crossovers.to_excel(writer, sheet_name='New Bullish Crossovers (MACD)', index=True)
            near_support.to_excel(writer, sheet_name='Stocks Currently Near Support', index=True)
        logging.info(f"Successfully saved all analysis results to '{excel_file_path}'")
    except Exception as e:
        logging.error(f"Failed to save Excel file: {e}")
        st.error(f"Failed to save Excel file: {e}")
        return results_df, None, None

    # --- Generate PDF Report ---
    if not REPORTLAB_INSTALLED:
        st.warning("`reportlab` is not installed. PDF report cannot be generated.")
        return results_df, excel_file_path, None

    try:
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        base_pdf_path = os.path.splitext(excel_file_path)[0]
        pdf_file_path = f"{base_pdf_path}_{timestamp}.pdf"

        doc = SimpleDocTemplate(pdf_file_path, pagesize=landscape(letter))
        elements = []
        styles = getSampleStyleSheet()

        elements.append(Paragraph(f"SPUS Analysis Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}", styles['h1']))
        elements.append(Spacer(1, 0.25*inch))

        elements.extend(create_pdf_table("Top 10 by Quant Score", top_10_quant, styles))
        elements.extend(create_pdf_table("Top 10 Undervalued (Graham)", top_10_undervalued, styles))
        elements.extend(create_pdf_table("New Bullish Crossovers (MACD)", new_crossovers, styles))
        elements.extend(create_pdf_table("Stocks Currently Near Support", near_support, styles))

        doc.build(elements)
        logging.info(f"PDF report saved to {pdf_file_path}")

    except Exception as e:
        logging.error(f"Failed to generate PDF report: {e}")
        st.error(f"Failed to generate PDF report: {e}")
        return results_df, excel_file_path, None

    # Clear status text
    status_text.empty()

    return results_df, excel_file_path, pdf_file_path

# ----------------------------------------------------------------------
# --- STREAMLIT UI ---
# ----------------------------------------------------------------------

# Load config first
CONFIG = load_config()

st.title("ðŸ“ˆ SPUS Stock Analyzer")

if CONFIG is None:
    st.error("**FATAL ERROR:** `config.json` not found.")
    st.info("Please make sure a valid `config.json` file is in the same directory as this web app.")
else:
    if not REPORTLAB_INSTALLED:
        st.warning("""
        **PDF Generation is OFF.** The `reportlab` library was not found.
        To enable PDF reports, please run: `pip install reportlab`
        """)

    # 1. File Uploader
    uploaded_file = st.file_uploader("Upload `TidalFG_Holdings_SPUS.csv`", type=["csv"])

    if uploaded_file is not None:

        # 2. Run Button
        if st.button("Run Analysis"):

            # 3. Call main logic
            try:
                # Pass the config and the uploaded file object
                results_df, excel_path, pdf_path = run_analysis(CONFIG, uploaded_file)

                if results_df is not None:
                    st.success("âœ… Analysis Complete!")

                    # 4. Display Results
                    st.subheader("Top 10 by Quant Score")
                    st.dataframe(results_df.head(10))

                    # 5. Download Buttons
                    col1, col2 = st.columns(2)

                    if excel_path:
                        with open(excel_path, "rb") as f_excel:
                            col1.download_button(
                                label="â¬‡ï¸ Download Excel Report",
                                data=f_excel,
                                file_name="spus_analysis_results.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )

                    if pdf_path:
                        with open(pdf_path, "rb") as f_pdf:
                            col2.download_button(
                                label="â¬‡ï¸ Download PDF Report",
                                data=f_pdf,
                                file_name=os.path.basename(pdf_path),
                                mime="application/pdf"
                            )

            except Exception as e:
                st.error(f"An unexpected error occurred: {e}")
                logging.error(f"Streamlit analysis error: {e}")