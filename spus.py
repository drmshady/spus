# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175PonR7Nc8EVaVO7AdmE3Fnu5S9OSq7r
"""

# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175PonR7Nc8EVaVO7AdmE3Fnu5S9OSq7r
"""

# -*- coding: utf-8 -*-
"""
SPUS Quantitative Analyzer v11 (Fibonacci Model)

This script performs a multi-factor quantitative analysis (Value, Momentum, Quality, Size)
on SPUS holdings. It now calculates dynamic Fibonacci retracement (61.8%)
and extension (161.8%) levels based on the recent 90-day high/low.
"""

import requests
import yfinance as yf
import pandas as pd
import pandas_ta as ta
import time
import os
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import openpyxl
from openpyxl.styles import Font
import io
import json
import requests.exceptions


# --- Function to load external config file ---
def load_config(path='config.json'):
    try:
        with open(path, 'r') as f:
            config = json.load(f)
        logging.info(f"Successfully loaded configuration from {path}")
        return config
    except FileNotFoundError:
        logging.error(f"FATAL: Configuration file '{path}' not found.")
        return None
    except json.JSONDecodeError:
        logging.error(f"FATAL: Could not decode JSON from '{path}'. Check for syntax errors.")
        return None
# ---------------------------------------------


# --- CONFIGURATION BLOCK ---
# CONFIG block removed, loaded from config.json
# --------------------------------

# --- NEW ROBUST FETCHER FUNCTION ---
def fetch_spus_tickers():
    '''
    Fetches SPUS holdings tickers by reading a local CSV file.
    This version reads from a predefined local path.
    '''
    # url = 'https://www.sp-funds.com/wp-content/uploads/data/TidalFG_Holdings_SPUS.csv' # Original URL commented out
    local_path = CONFIG['SPUS_HOLDINGS_CSV_PATH']
    ticker_column_name = 'StockTicker' # Correct column name

    if not os.path.exists(local_path):
        logging.error(f"Local SPUS holdings file not found at: {local_path}")
        return []

    try:
        # Try to let pandas handle the parsing directly
        holdings_df = pd.read_csv(local_path)

    except pd.errors.ParserError:
        # If default parsing fails, it's likely due to metadata at the top.
        # Let's try skipping the first few lines.
        logging.warning("Pandas ParserError. Trying again with 'skiprows'...")
        for i in range(1, 10): # Try skipping up to 9 lines
            try:
                holdings_df = pd.read_csv(local_path, skiprows=i)
                if ticker_column_name in holdings_df.columns:
                    logging.info(f"Successfully parsed CSV by skipping {i} rows.")
                    break # Found the header
            except Exception:
                continue # Try next skip value
        else:
            # This 'else' belongs to the 'for' loop
            logging.error(f"Failed to parse CSV from {local_path} even after skipping 9 rows.")
            return []

    except Exception as e:
        logging.error(f"An unexpected error occurred during local CSV read/parse: {e}")
        return []

    # --- Post-processing: Clean the DataFrame ---

    if ticker_column_name not in holdings_df.columns:
        logging.error(f"CSV from {local_path} downloaded, but '{ticker_column_name}' column not found.")
        return []

    # Clean metadata from the *bottom* of the file (e.g., "Total" rows)
    # These often appear as NaN in the 'StockTicker' column.
    try:
        # Ensure 'StockTicker' column is string type before checking for NaN
        holdings_df[ticker_column_name] = holdings_df[ticker_column_name].astype(str)
        first_nan_index = holdings_df[holdings_df[ticker_column_name].isna()].index[0]
        holdings_df = holdings_df.iloc[:first_nan_index]
        logging.info("Removed footer metadata from CSV.")
    except IndexError:
        # No NaN rows found, assume file is clean
        pass
    except Exception as e:
         logging.warning(f"Error cleaning footer metadata: {e}")
         pass # Continue even if cleaning fails


    ticker_symbols = holdings_df[ticker_column_name].tolist()

    # Final cleaning of the list
    ticker_symbols = [s for s in ticker_symbols if isinstance(s, str) and s and s != ticker_column_name and 'CASH' not in s]

    logging.info(f"Successfully fetched {len(ticker_symbols)} ticker symbols for SPUS from local file.")
    return ticker_symbols
# --- END NEW FUNCTION ---


# 2. Define helper functions

# --- ⭐️ UPDATED Support/Resistance Function to include Dates and Fib Levels ---
def calculate_support_resistance(hist_df):
    """
    Calculates Support, Resistance, Dates, and Fibonacci levels
    based on the lookback period.
    """
    if hist_df is None or hist_df.empty:
        return None, None, None, None, None, None

    try:
        lookback_period = CONFIG.get('SR_LOOKBACK_PERIOD', 90)

        if len(hist_df) < lookback_period:
            recent_hist = hist_df
        else:
            recent_hist = hist_df.iloc[-lookback_period:]

        # Find Support (min low) and its date
        support_val = recent_hist['Low'].min()
        support_date = recent_hist['Low'].idxmin()

        # Find Resistance (max high) and its date
        resistance_val = recent_hist['High'].max()
        resistance_date = recent_hist['High'].idxmax()

        # Calculate Fibonacci Levels
        fib_61_8_level = None
        fib_161_8_level = None

        high_low_diff = resistance_val - support_val
        if high_low_diff > 0:
            # Fib 61.8% Retracement (Support Level)
            fib_61_8_level = resistance_val - (high_low_diff * 0.618)

            # Fib 161.8% Extension (Profit Target)
            fib_161_8_level = resistance_val + (high_low_diff * 0.618)

        return support_val, support_date, resistance_val, resistance_date, fib_61_8_level, fib_161_8_level

    except Exception as e:
        logging.warning(f"Error in calculate_support_resistance: {e}. Defaulting to long-term S/R.")
        # Fallback to the old method in case of error
        support_val = hist_df['Low'].min()
        support_date = hist_df['Low'].idxmin()
        resistance_val = hist_df['High'].max()
        resistance_date = hist_df['High'].idxmax()
        return support_val, support_date, resistance_val, resistance_date, None, None
# --- ⭐️ END UPDATED Function ---


def calculate_financials_and_fair_price(ticker_obj, last_price, ticker):
    """
    Calculates financials. NOW checks for a cached .json file
    for the `.info` data before making a network request.
    """
    info = {}
    cache_path = os.path.join(CONFIG['INFO_CACHE_DIR'], f"{ticker}.json")
    cache_duration_seconds = CONFIG['INFO_CACHE_DURATION_HOURS'] * 3600

    try:
        # Check if a fresh cache file exists
        if os.path.exists(cache_path) and (time.time() - os.path.getmtime(cache_path) < cache_duration_seconds):
            with open(cache_path, 'r') as f:
                info = json.load(f)
            logging.info(f"[{ticker}] Loaded fundamental data from cache.")
        else:
            # If no cache or cache is stale, fetch from network
            logging.info(f"[{ticker}] Fetching fundamental data from network...")
            info = ticker_obj.info
            # Save the new data to cache
            with open(cache_path, 'w') as f:
                json.dump(info, f, indent=4)

        # --- Now, proceed with the 'info' dictionary ---
        pe_ratio = info.get('forwardPE', None)
        pb_ratio = info.get('priceToBook', None)
        div_yield = info.get('dividendYield', None)
        debt_to_equity = info.get('debtToEquity', None)
        rev_growth = info.get('revenueGrowth', None)
        roe = info.get('returnOnEquity', None)
        ev_ebitda = info.get('enterpriseToEbitda', None)
        ps_ratio = info.get('priceToSalesTrailing12Months', None)
        high_52wk = info.get('fiftyTwoWeekHigh', None)
        low_52wk = info.get('fiftyTwoWeekLow', None)
        market_cap = info.get('marketCap', None)
        sector = info.get('sector', 'N/A') # --- ADDED SECTOR ---

        eps = info.get('trailingEps', None)
        bvps = None
        graham_number = None
        valuation_signal = "N/A"

        if eps is not None and pb_ratio is not None and eps > 0 and pb_ratio > 0 and last_price is not None:
            bvps = last_price / pb_ratio
            graham_number = (22.5 * eps * bvps) ** 0.5

            if last_price < graham_number:
                valuation_signal = "Undervalued (Graham)"
            else:
                valuation_signal = "Overvalued (Graham)"
        elif eps is not None and eps <= 0:
            valuation_signal = "Unprofitable (EPS < 0)"

        financial_dict = {
            'Forward P/E': pe_ratio,
            'P/B Ratio': pb_ratio,
            'Market Cap': market_cap,
            'Sector': sector, # --- ADDED SECTOR ---
            'Dividend Yield': div_yield * 100 if div_yield else None,
            'Debt/Equity': debt_to_equity,
            'Revenue Growth (QoQ)': rev_growth * 100 if rev_growth else None,
            'Return on Equity (ROE)': roe * 100 if roe else None,
            'EV/EBITDA': ev_ebitda,
            'Price/Sales (P/S)': ps_ratio,
            '52 Week High': high_52wk,
            '52 Week Low': low_52wk,
            'Graham Number': graham_number,
            'Valuation (Graham)': valuation_signal
        }
        return financial_dict

    except Exception as e:
        logging.error(f"Error fetching/processing fundamental data for {ticker}: {e}")
        # Return all keys as None
        # --- ADDED SECTOR TO NULL DICT ---
        return {key: None for key in ['Forward P/E', 'P/B Ratio', 'Market Cap', 'Sector', 'Dividend Yield', 'Debt/Equity', 'Revenue Growth (QoQ)', 'Graham Number', 'Valuation (Graham)', 'Return on Equity (ROE)', 'EV/EBITDA', 'Price/Sales (P/S)', '52 Week High', '52 Week Low']}
# --- ⭐️ END UPDATED FUNCTION 1 ---


# Old calculate_quant_score function removed (replaced by Z-Score ranking).

# 3. Dedicated function for data fetching
# --- ⭐️ UPDATED FUNCTION 2: ADDED NEWS & CALENDAR + BUG FIX ---
def process_ticker(ticker):
    """
    Processes a single ticker. Fetches history (cached),
    fundamentals (cached), and live news/calendar.
    """

    null_return = {
        'ticker': ticker, 'momentum': None, 'rsi': None, 'last_price': None,
        'support_resistance': None, 'trend': None, 'macd': None, 'signal_line': None,
        'hist_val': None, 'macd_signal': None, 'financial_dict': None, 'success': False,
        'recent_news': "N/A", 'latest_headline': "N/A", 'earnings_date': "N/A" # --- ADDED ---
    }

    file_path = os.path.join(CONFIG['HISTORICAL_DATA_DIR'], f'{ticker}.feather')
    existing_hist_df = pd.DataFrame()
    start_date = None

    if os.path.exists(file_path):
        try:
            existing_hist_df = pd.read_feather(file_path)
            existing_hist_df['Date'] = pd.to_datetime(existing_hist_df['Date'])
            existing_hist_df.set_index('Date', inplace=True)

            if not existing_hist_df.empty:
                existing_hist_df.index = pd.to_datetime(existing_hist_df.index, utc=True)
                last_date = existing_hist_df.index.max()
                start_date = last_date + timedelta(days=1)
        except Exception as e:
            logging.warning(f"Error loading existing FEATHER data for {ticker}: {e}")
            existing_hist_df = pd.DataFrame()

    new_hist = pd.DataFrame()
    ticker_obj = None
    fetch_success = False

    # --- ⭐️ FIX: Check if cache is already up-to-date ---
    today = datetime.now().date()
    should_fetch = True
    if start_date:
        # If the calculated start_date is *after* today, don't fetch
        if start_date.date() > today:
            logging.info(f"[{ticker}] History cache is already up-to-date (Last: {start_date.date() - timedelta(days=1)}). Skipping network fetch.")
            should_fetch = False

    if should_fetch:
        for attempt in range(3):
            try:
                ticker_obj = yf.Ticker(ticker)
                if start_date:
                    # Fetch from start_date (which is in the past or today)
                    new_hist = ticker_obj.history(start=start_date.strftime('%Y-%m-%d'))
                else:
                    # Fetch full history
                    new_hist = ticker_obj.history(period=CONFIG['HISTORICAL_DATA_PERIOD'])
                fetch_success = True
                break
            except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout, TimeoutError) as e:
                logging.warning(f"[{ticker}] Network error on attempt {attempt+1}/3: {e}. Retrying in {5*(attempt+1)}s...")
                time.sleep(5 * (attempt+1))
            except Exception as e:
                # This catches the yfinance "start date after end date" error if it still occurs
                logging.error(f"Error fetching new history for {ticker}: {e}")
                break
    # --- ⭐️ END FIX ---

    if not fetch_success and existing_hist_df.empty:
        logging.error(f"[{ticker}] Failed to fetch history and no cache exists.")
        return null_return

    # --- ⭐️ FIX 2: Handle concat warning ---
    # Only concat if new_hist actually has data.
    if not new_hist.empty:
        combined_hist = pd.concat([existing_hist_df, new_hist]).drop_duplicates(keep='last').sort_index()
    else:
        combined_hist = existing_hist_df
    # --- ⭐️ END FIX 2 ---

    if combined_hist.empty:
        # This can happen if the ticker is delisted (e.g., AOS) and we have no cache
        logging.warning(f"[{ticker}] No history data found (possibly delisted).")
        return null_return

    if not combined_hist.index.is_unique:
         duplicate_dates = combined_hist.index[combined_hist.index.duplicated(keep='first')]
         logging.warning(f"Removed duplicate dates for {ticker} before calculating TA: {duplicate_dates.tolist()}")
         combined_hist = combined_hist[~combined_hist.index.duplicated(keep='first')]

    hist = combined_hist

    momentum = None
    rsi = None
    last_price = None
    support_resistance = None
    trend = 'Insufficient data for trend'

    try:
        hist.ta.rsi(length=CONFIG['RSI_WINDOW'], append=True)
        hist.ta.sma(length=CONFIG['SHORT_MA_WINDOW'], append=True)
        hist.ta.sma(length=CONFIG['LONG_MA_WINDOW'], append=True)
        hist.ta.macd(fast=CONFIG['MACD_SHORT_SPAN'], slow=CONFIG['MACD_LONG_SPAN'], signal=CONFIG['MACD_SIGNAL_SPAN'], append=True)
    except Exception as e:
        logging.warning(f"Error calculating TA for {ticker}: {e}. Skipping TA.")
        pass

    rsi_col = f'RSI_{CONFIG["RSI_WINDOW"]}'
    short_ma_col = f'SMA_{CONFIG["SHORT_MA_WINDOW"]}'
    long_ma_col = f'SMA_{CONFIG["LONG_MA_WINDOW"]}'
    macd_col = f'MACD_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'
    macd_h_col = f'MACDh_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'
    macd_s_col = f'MACDs_{CONFIG["MACD_SHORT_SPAN"]}_{CONFIG["MACD_LONG_SPAN"]}_{CONFIG["MACD_SIGNAL_SPAN"]}'

    if not hist.empty:
        last_price = hist['Close'].iloc[-1]

        min_data_points = CONFIG['MIN_DATA_POINTS_MOMENTUM']
        if len(hist) >= min_data_points:
            try:
                price_now = hist['Close'].iloc[-1]
                # --- ⭐️ SYNTAX FIX IS HERE ---
                price_year_ago = hist['Close'].iloc[-min_data_points]
                # --- ⭐️ END FIX ---
                momentum = ((price_now - price_year_ago) / price_year_ago) * 100
            except IndexError:
                pass

        rsi = hist[rsi_col].iloc[-1] if rsi_col in hist.columns else None
        last_short_ma = hist[short_ma_col].iloc[-1] if short_ma_col in hist.columns else None
        last_long_ma = hist[long_ma_col].iloc[-1] if long_ma_col in hist.columns else None

        macd = hist[macd_col].iloc[-1] if macd_col in hist.columns else None
        hist_val = hist[macd_h_col].iloc[-1] if macd_h_col in hist.columns else None
        signal_line = hist[macd_s_col].iloc[-1] if macd_s_col in hist.columns else None

        trend = 'No Clear Trend'
        if not pd.isna(last_short_ma) and not pd.isna(last_long_ma) and not pd.isna(last_price):
            if last_short_ma > last_long_ma:
                if last_price > last_short_ma:
                    trend = 'Confirmed Uptrend'
                else:
                    trend = 'Uptrend (Correction)'
            elif last_short_ma < last_long_ma:
                if last_price < last_short_ma:
                    trend = 'Confirmed Downtrend'
                else:
                    trend = 'Downtrend (Rebound)'

        macd_signal = "N/A"
        if macd_h_col in hist.columns and len(hist) >= 2 and not pd.isna(hist_val):
            prev_hist = hist[macd_h_col].iloc[-2]
            if not pd.isna(prev_hist):
                if hist_val > 0 and prev_hist <= 0:
                    macd_signal = "Bullish Crossover (Favorable)"
                elif hist_val < 0 and prev_hist >= 0:
                    macd_signal = "Bearish Crossover (Unfavorable)"
                elif hist_val > 0:
                    macd_signal = "Bullish (Favorable)"
                elif hist_val < 0:
                    macd_signal = "Bearish (Unfavorable)"

        # --- ⭐️ MODIFIED S/R CALL ---
        support, support_date, resistance, resistance_date, fib_61_8, fib_161_8 = calculate_support_resistance(hist)

        # Format dates
        support_date_str = support_date.strftime('%Y-%m-%d') if pd.notna(support_date) else "N/A"
        resistance_date_str = resistance_date.strftime('%Y-%m-%d') if pd.notna(resistance_date) else "N/A"

        if support is not None and resistance is not None:
            support_resistance = {
                'Support': support,
                'Support_Date': support_date_str,
                'Resistance': resistance,
                'Resistance_Date': resistance_date_str,
                'Fib_61_8': fib_61_8,
                'Fib_161_8': fib_161_8
            }
        # --- ⭐️ END MODIFICATION ---


        # We need a ticker_obj to get financials. If we skipped fetch, create it.
        if ticker_obj is None:
            ticker_obj = yf.Ticker(ticker)

        financial_dict = calculate_financials_and_fair_price(ticker_obj, last_price, ticker)

        # --- ⭐️ NEW: Fetch News and Calendar Events ---
        recent_news_flag = "No"
        latest_headline = "N/A"
        earnings_date = "N/A"
        try:
            # Fetch news
            news = ticker_obj.news
            if news:
                latest_headline = news[0].get('title', "N/A")
                now_ts = datetime.now().timestamp()
                # 48 hours = 172800 seconds
                forty_eight_hours_ago_ts = now_ts - 172800

                for item in news:
                    if item.get('providerPublishTime', 0) > forty_eight_hours_ago_ts:
                        recent_news_flag = "Yes"
                        break # Found one, no need to keep checking

            # Fetch calendar
            calendar = ticker_obj.calendar
            # --- ⭐️ BUG FIX: Check if calendar is a non-empty dictionary ---
            if calendar and isinstance(calendar, dict) and 'Earnings Date' in calendar and calendar['Earnings Date']:
                # Get the first date from the list
                date_val = calendar['Earnings Date'][0]

                # Now format the date
                if isinstance(date_val, str):
                    earnings_date = date_val
                elif hasattr(date_val, 'strftime'): # Handle timestamp
                    earnings_date = date_val.strftime('%Y-%m-%d')
                else:
                    earnings_date = str(date_val)
            # --- ⭐️ END BUG FIX ---

        except Exception as e:
            logging.warning(f"[{ticker}] Error fetching news or calendar: {e}")
        # --- ⭐️ END NEW BLOCK ---


        try:
            # Only write to cache if we actually fetched new data
            if not new_hist.empty:
                combined_hist.reset_index().to_feather(file_path)
        except Exception as e:
            logging.error(f"[{ticker}] Failed to save Feather cache: {e}")

        return {
            'ticker': ticker,
            'momentum': momentum,
            'rsi': rsi,
            'last_price': last_price,
            'support_resistance': support_resistance,
            'trend': trend,
            'macd': macd,
            'signal_line': signal_line,
            'hist_val': hist_val,
            'macd_signal': macd_signal,
            'financial_dict': financial_dict,
            'recent_news': recent_news_flag,      # --- ADDED ---
            'latest_headline': latest_headline,  # --- ADDED ---
            'earnings_date': earnings_date,        # --- ADDED ---
            'success': True
        }

    return null_return
# --- ⭐️ END UPDATED FUNCTION 2 ---


# Main execution block
if __name__ == "__main__":

    # --- Load Configuration ---
    CONFIG = load_config()
    if CONFIG is None:
        logging.error("Exiting due to missing or invalid config.json.")
        exit()
    # ---------------------------

    # --- ⭐️ NEW: DEFINE MAX RISK FOR POSITION SIZING ---
    MAX_RISK_USD = 50
    # ---------------------------------------------------

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(CONFIG['LOG_FILE_PATH']),
            logging.StreamHandler()
        ]
    )

    # This block will try to mount Google Drive, and fail gracefully
    # with a warning if not in Colab, which is what we want.
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        logging.info("Google Drive mounted successfully.")
    except:
        if os.environ.get('COLAB_GPU') is None:
                logging.warning("Running outside Google Colab. Google Drive mount skipped.")
        else:
                logging.info("Google Drive already mounted or running outside Colab.")


    # --- Call the new SPUS function ---
    logging.info("Starting SPUS Ticker Fetch...")
    ticker_symbols = fetch_spus_tickers()
    # --- END ---

    if not ticker_symbols:
        logging.warning("No ticker symbols fetched. Exiting.")
        exit()

    exclude_tickers = CONFIG['EXCLUDE_TICKERS']
    ticker_symbols = [ticker for ticker in ticker_symbols if ticker not in exclude_tickers]
    logging.info(f"Excluded {len(exclude_tickers)} tickers: {', '.join(exclude_tickers)}")

    if CONFIG['TICKER_LIMIT'] > 0:
        ticker_symbols = ticker_symbols[:CONFIG['TICKER_LIMIT']]
        logging.info(f"Limiting analysis to the first {CONFIG['TICKER_LIMIT']} tickers for testing.")


    # --- ⭐️ CHANGE: Create *both* cache directories ---
    historical_data_dir = CONFIG['HISTORICAL_DATA_DIR']
    if not os.path.exists(historical_data_dir):
        os.makedirs(historical_data_dir)
        logging.info(f"Created '{historical_data_dir}' directory.")
    else:
        logging.info(f"'{historical_data_dir}' directory already exists.")

    info_cache_dir = CONFIG['INFO_CACHE_DIR']
    if not os.path.exists(info_cache_dir):
        os.makedirs(info_cache_dir)
        logging.info(f"Created '{info_cache_dir}' directory.")
    else:
        logging.info(f"'{info_cache_dir}' directory already exists.")
    # --- ⭐️ END CHANGE ---


    momentum_data = {}
    rsi_data = {}
    last_prices = {}
    support_resistance_levels = {}
    trend_data = {}
    macd_data = {}
    financial_data = {}
    processed_tickers = set()

    # --- ⭐️ NEW: Dictionaries for new data ---
    news_data = {}
    headline_data = {}
    calendar_data = {}
    # --- ⭐️ END NEW ---

    MAX_WORKERS = CONFIG['MAX_CONCURRENT_WORKERS']

    logging.info(f"\nStarting concurrent data fetch, analysis, and financials with {MAX_WORKERS} workers...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_ticker = {
            executor.submit(process_ticker, ticker): ticker
            for ticker in ticker_symbols
        }

        for i, future in enumerate(as_completed(future_to_ticker)):
            ticker = future_to_ticker[future]
            try:
                result = future.result(timeout=60)

                if result['success']:
                    ticker = result['ticker']
                    processed_tickers.add(ticker)

                    if result['momentum'] is not None:
                        momentum_data[ticker] = result['momentum']
                    if result['rsi'] is not None:
                        rsi_data[ticker] = result['rsi']
                    if result['last_price'] is not None:
                        last_prices[ticker] = result['last_price']
                    if result['support_resistance'] is not None:
                        support_resistance_levels[ticker] = result['support_resistance']

                    trend_data[ticker] = result['trend']

                    macd_info = macd_data.get(ticker, {
                        'MACD': 'N/A', 'Signal_Line': 'N/A', 'Histogram': 'N/A', 'Signal': 'N/A'
                    })
                    macd_data[ticker] = macd_info

                    if result['macd'] is not None:
                         macd_data[ticker]['MACD'] = result['macd']
                    if result['signal_line'] is not None:
                         macd_data[ticker]['Signal_Line'] = result['signal_line']
                    if result['hist_val'] is not None:
                         macd_data[ticker]['Histogram'] = result['hist_val']
                    if result['macd_signal'] is not None:
                         macd_data[ticker]['Signal'] = result['macd_signal']

                    financial_data[ticker] = result['financial_dict']

                    # --- ⭐️ NEW: Get new data from result ---
                    news_data[ticker] = result['recent_news']
                    headline_data[ticker] = result['latest_headline']
                    calendar_data[ticker] = result['earnings_date']
                    # --- ⭐️ END NEW ---


                if (i + 1) % 50 == 0 or (i + 1) == len(ticker_symbols):
                    elapsed = time.time() - start_time
                    logging.info(f"Processed {i + 1}/{len(ticker_symbols)} tickers. Elapsed time: {elapsed:.2f} seconds.")

            except TimeoutError:
                 logging.error(f"Processing {ticker} timed out after 60 seconds.")
            except Exception as e:
                logging.error(f"Error processing {ticker} in main loop: {e}")

    end_time = time.time()
    logging.info(f"\nFinished concurrent processing. Total time: {end_time - start_time:.2f} seconds.")
    logging.info(f"Processed data for {len(processed_tickers)} tickers.")
    logging.info(f"Sufficient data for momentum/analysis for {len(momentum_data)} tickers.")


    # --- ⭐️ SECTION UPDATED: Calculate Risk/Reward ---
    threshold_percentage = CONFIG['PRICE_THRESHOLD_PERCENT']
    comparison_results = {}
    support_diff_percentages = {}
    resistance_diff_percentages = {}

    # --- ⭐️ NEW: Dictionaries for Risk/Reward data ---
    risk_percentages = {}
    reward_percentages = {}
    risk_reward_ratios = {}

    for ticker in last_prices.keys():
        last_price = last_prices.get(ticker)
        levels = support_resistance_levels.get(ticker)

        # Set defaults
        comparison_results[ticker] = 'Price or S/R levels not available'
        support_diff_percentages[ticker] = "N/A"
        resistance_diff_percentages[ticker] = "N/A"
        risk_percentages[ticker] = "N/A"
        reward_percentages[ticker] = "N/A"
        risk_reward_ratios[ticker] = "N/A"

        if last_price is not None and levels is not None and last_price > 0:
            support = levels.get('Support')
            resistance = levels.get('Resistance')

            if support is not None and resistance is not None and resistance > support:
                support_diff = last_price - support
                resistance_diff = resistance - last_price

                # --- ⭐️ NEW: Calculate Risk/Reward Pct and Ratio ---
                risk_pct = (support_diff / last_price) * 100
                reward_pct = (resistance_diff / last_price) * 100

                risk_percentages[ticker] = risk_pct
                reward_percentages[ticker] = reward_pct

                # Only calculate R/R if risk is positive (stop loss is below price)
                if risk_pct > 0:
                    risk_reward_ratios[ticker] = reward_pct / risk_pct
                else:
                    risk_reward_ratios[ticker] = "N/A (Price below Support)"
                # --- ⭐️ END NEW ---

                support_diff_percentage = ((last_price - support) / support) * 100 if support != 0 else float('inf')
                resistance_diff_percentage = ((last_price - resistance) / resistance) * 100 if resistance != 0 else float('inf')

                support_diff_percentages[ticker] = support_diff_percentage
                resistance_diff_percentages[ticker] = resistance_diff_percentage

                if abs(support_diff_percentage) <= threshold_percentage:
                    comparison_results[ticker] = 'Near Support'
                elif abs(resistance_diff_percentage) <= threshold_percentage:
                    comparison_results[ticker] = 'Near Resistance'
                elif last_price > resistance:
                    comparison_results[ticker] = 'Above Resistance'
                elif last_price < support:
                    comparison_results[ticker] = 'Below Support'
                else:
                    comparison_results[ticker] = 'Between Support and Resistance'

    logging.info("\nSummary of Last Price vs. Support/Resistance Levels (first 50):")
    for i, (ticker, result) in enumerate(comparison_results.items()):
        if i >= 50:
            break
        logging.info(f"{ticker}: {result}")
    # --- ⭐️ END SECTION UPDATE ---


    # --- 5. Aggregate and save results ---
    logging.info("\n--- Aggregating, Scoring, and Building Report ---")

    tickers_to_report = list(momentum_data.keys())

    if not tickers_to_report:
        logging.warning("No tickers had sufficient data.")
    else:
        results_list = []
        for ticker in tickers_to_report:
            momentum = momentum_data.get(ticker, "N/A")
            rsi = rsi_data.get(ticker, "N/A")
            last_price = last_prices.get(ticker, "N/A")

            # --- ⭐️ MODIFIED: Unpack new S/R data ---
            support_resistance = support_resistance_levels.get(ticker, {
                "Support": "N/A", "Resistance": "N/A", "Support_Date": "N/A",
                "Resistance_Date": "N/A", "Fib_61_8": "N/A", "Fib_161_8": "N/A"
            })
            # --- ⭐️ END MODIFICATION ---

            trend = trend_data.get(ticker, "N/A")
            price_vs_levels = comparison_results.get(ticker, "N/A")

            # --- ⭐️ CHANGED: Get new Risk/Reward data ---
            pct_to_support = risk_percentages.get(ticker, "N/A") # Use new risk_percentages dict
            pct_to_resistance = reward_percentages.get(ticker, "N/A") # Use new reward_percentages dict
            rr_ratio = risk_reward_ratios.get(ticker, "N/A")

            macd_info = macd_data.get(ticker, {
                'MACD': 'N/A', 'Signal_Line': 'N/A', 'Histogram': 'N/A', 'Signal': 'N/A'
            })

            # --- ⭐️ ADDED 'Sector' to default fin_info ---
            fin_info = financial_data.get(ticker, {
                'Forward P/E': 'N/A', 'P/B Ratio': 'N/A', 'Market Cap': 'N/A', 'Sector': 'N/A', 'PEG Ratio': 'N/A',
                'Dividend Yield': 'N/A', 'Debt/Equity': 'N/A', 'Revenue Growth (QoQ)': 'N/A',
                'Graham Number': 'N/A', 'Valuation (Graham)': 'N/A',
                'Return on Equity (ROE)': 'N/A', 'EV/EBITDA': 'N/A', 'Price/Sales (P/S)': 'N/A',
                '52 Week High': 'N/A', '52 Week Low': 'N/A'
            })

            # --- ⭐️ NEW: Get new event data ---
            recent_news = news_data.get(ticker, "N/A")
            latest_headline = headline_data.get(ticker, "N/A")
            earnings_date = calendar_data.get(ticker, "N/A")

            # Format percentages
            try: momentum_str = f"{momentum:.2f}"
            except: momentum_str = "N/A"
            try: pct_to_support_str = f"{pct_to_support:.2f}"
            except: pct_to_support_str = "N/A"
            try: pct_to_resistance_str = f"{pct_to_resistance:.2f}"
            except: pct_to_resistance_str = "N/A"
            try: div_yield_str = f"{fin_info['Dividend Yield']:.2f}"
            except: div_yield_str = "N/A"
            try: rev_growth_str = f"{fin_info['Revenue Growth (QoQ)']:.2f}"
            except: rev_growth_str = "N/A"
            try: rr_ratio_str = f"{rr_ratio:.2f}"
            except: rr_ratio_str = "N/A"

            # --- ⭐️ NEW: Calculate Shares to Buy ---
            shares_to_buy_str = "N/A"
            try:
                # Convert to numeric, errors='coerce' makes failures NaN
                last_price_num = pd.to_numeric(last_price, errors='coerce')
                support_price_num = pd.to_numeric(support_resistance.get('Support'), errors='coerce')

                if pd.notna(last_price_num) and pd.notna(support_price_num):
                    risk_per_share = last_price_num - support_price_num

                    # Only calculate if risk is positive (price > support) and not zero
                    if risk_per_share > 0:
                        shares_to_buy = MAX_RISK_USD / risk_per_share
                        shares_to_buy_str = f"{shares_to_buy:.2f}" # Format as string
                    elif risk_per_share <= 0:
                        shares_to_buy_str = "N/A (Price below Support)"
            except Exception:
                pass # Keep as "N/A"
            # --- ⭐️ END NEW ---


            # --- ⭐️ CHANGED: Added new fields to result_data ---
            result_data = {
                'Ticker': ticker,
                'Valuation (Graham)': fin_info.get('Valuation (Graham)'),
                'MACD_Signal': macd_info.get('Signal'),
                'Trend (50/200 Day MA)': trend,
                'Price vs. Levels': price_vs_levels,
                'Last Price': last_price,

                'Cut Loss Level (Support)': support_resistance.get('Support'),
                'Date of Support': support_resistance.get('Support_Date'),       # --- ADDED ---
                'Take Profit (Resistance)': support_resistance.get('Resistance'),
                'Date of Resistance': support_resistance.get('Resistance_Date'),   # --- ADDED ---
                'Fib 61.8% Support': support_resistance.get('Fib_61_8'),      # --- ADDED ---
                'Fib 161.8% Target': support_resistance.get('Fib_161_8'),     # --- ADDED ---

                'Risk % (to Support)': pct_to_support_str,
                'Reward % (to Resist)': pct_to_resistance_str,
                'Risk/Reward Ratio': rr_ratio_str,
                'Shares to Buy ($50 Risk)': shares_to_buy_str, # --- ADDED ---

                'Recent News (48h)': recent_news,      # --- ADDED ---
                'Next Earnings Date': earnings_date,       # --- ADDED ---
                'Latest Headline': latest_headline,    # --- ADDED ---

                'Fair Price (Graham)': fin_info.get('Graham Number'),
                'Forward P/E': fin_info.get('Forward P/E'),
                'P/B Ratio': fin_info.get('P/B Ratio'),
                'Market Cap': fin_info.get('Market Cap'),
                'Sector': fin_info.get('Sector'),
                'Dividend Yield (%)': div_yield_str,
                'Debt/Equity': fin_info.get('Debt/Equity'),
                'Revenue Growth (QoQ)': rev_growth_str,
                '1-Year Momentum (%)': momentum_str,
                'RSI (14-day)': rsi,
                'Return on Equity (ROE)': fin_info.get('Return on Equity (ROE)'),
                'EV/EBITDA': fin_info.get('EV/EBITDA'),
                'Price/Sales (P/S)': fin_info.get('Price/Sales (P/S)'),
                '52 Week High': fin_info.get('52 Week High'),
                '52 Week Low': fin_info.get('52 Week Low')
            }

            results_list.append(result_data)

        # --- Create Final DataFrame ---
        # --- ⭐️ CHANGED: Added new columns to columns_order ---
        columns_order = [
            'Final Quant Score', 'Z_Value', 'Z_Momentum', 'Z_Quality', 'Z_Size', 'Value_Discount', 'Ticker', 'Sector',
            'Last Price', 'Cut Loss Level (Support)', 'Take Profit (Resistance)',
            'Date of Support', 'Date of Resistance', 'Fib 61.8% Support', 'Fib 161.8% Target',
            'Risk % (to Support)', 'Reward % (to Resist)', 'Risk/Reward Ratio', 'Shares to Buy ($50 Risk)',
            'Recent News (48h)', 'Next Earnings Date', 'Latest Headline',
            'Valuation (Graham)', 'MACD_Signal', 'Trend (50/200 Day MA)',
            'Price vs. Levels', 'Fair Price (Graham)', 'Forward P/E', 'P/B Ratio', 'Market Cap',
            'Dividend Yield (%)', 'Debt/Equity', 'Revenue Growth (QoQ)',
            '1-Year Momentum (%)', 'RSI (14-day)',
            'Return on Equity (ROE)', 'EV/EBITDA', 'Price/Sales (P/S)',
            '52 Week High', '52 Week Low'
        ]

        # Remove old S/R columns if they exist by mistake
        columns_to_remove = ['Support', 'Resistance', '% to Support', '% to Resistance']
        columns_order = [col for col in columns_order if col not in columns_to_remove]


        results_df = pd.DataFrame(results_list, columns=columns_order)

        # --- ⭐️ NEW: Sector-Neutral Z-Score Logic ---
        logging.info("Calculating new Sector-Neutral Z-Score-based multi-factor scores...")

        FACTOR_WEIGHTS = {
            'VALUE': 0.40,
            'MOMENTUM': 0.30,
            'QUALITY': 0.20,
            'SIZE': 0.10
        }

        # --- Helper function to calculate Z-Score within a group ---
        def calculate_sector_zscore(series):
            series = pd.to_numeric(series, errors='coerce')
            # Fill NaNs with the *sector's* mean
            series = series.fillna(series.mean())
            if series.std() == 0: # Avoid division by zero if all values in sector are same
                return 0
            return (series - series.mean()) / series.std()

        # Handle potential division by zero or non-numeric errors
        graham_price = pd.to_numeric(results_df['Fair Price (Graham)'], errors='coerce')
        last_price_pd = pd.to_numeric(results_df['Last Price'], errors='coerce')

        # Calculate Value Discount (higher is better)
        last_price_safe = last_price_pd.replace(0, pd.NA)
        results_df['Value_Discount'] = graham_price / last_price_safe

        # Calculate Z-Scores
        # fillna(0) at the end catches any *groups* that failed (e.g., a sector with one 'N/A' stock)

        # Value Score (higher discount is better)
        results_df['Z_Value'] = results_df.groupby('Sector')['Value_Discount'].transform(calculate_sector_zscore).fillna(0)

        # Momentum Score (higher momentum is better)
        results_df['Z_Momentum'] = results_df.groupby('Sector')['1-Year Momentum (%)'].transform(calculate_sector_zscore).fillna(0)

        # Quality Score (higher ROE is better)
        results_df['Z_Quality'] = results_df.groupby('Sector')['Return on Equity (ROE)'].transform(calculate_sector_zscore).fillna(0)

        # Size Score (lower market cap is better)
        # We need to *reverse* the Z-score for size, so multiply by -1
        # --- ⭐️ MODIFICATION: Convert Market Cap to numeric *before* Z-score ---
        results_df['Market Cap'] = pd.to_numeric(results_df['Market Cap'], errors='coerce')
        results_df['Z_Size'] = results_df.groupby('Sector')['Market Cap'].transform(calculate_sector_zscore).fillna(0) * -1
        # --- ⭐️ END MODIFICATION ---

        # Calculate Final Weighted Score
        results_df['Final Quant Score'] = (
            (results_df['Z_Value'] * FACTOR_WEIGHTS['VALUE']) +
            (results_df['Z_Momentum'] * FACTOR_WEIGHTS['MOMENTUM']) +
            (results_df['Z_Quality'] * FACTOR_WEIGHTS['QUALITY']) +
            (results_df['Z_Size'] * FACTOR_WEIGHTS['SIZE'])
        )

        logging.info("Finished calculating new Z-scores.")
        # --- ⭐️ END NEW LOGIC ---

        # --- ⭐️ NEW: Must convert R/R Ratio to numeric *after* it's populated ---
        results_df['Risk/Reward Ratio'] = pd.to_numeric(results_df['Risk/Reward Ratio'], errors='coerce')


        results_df.set_index('Ticker', inplace=True)

        results_df.sort_values(by='Final Quant Score', ascending=False, inplace=True)

        # --- ⭐️ DEFINE ALL DATAFRAMES FOR REPORT ---
        # --- ⭐️ NEW: Added Top 10 Market Cap table ---
        top_10_market_cap = results_df.sort_values(by='Market Cap', ascending=False).head(10)
        # --- ⭐️ END NEW ---

        top_20_quant = results_df.head(20)
        top_10_undervalued = results_df[results_df['Valuation (Graham)'] == 'Undervalued (Graham)'].sort_values(by='Final Quant Score', ascending=False).head(10)
        new_crossovers = results_df[results_df['MACD_Signal'] == 'Bullish Crossover (Favorable)'].sort_values(by='Final Quant Score', ascending=False).head(10)
        near_support = results_df[results_df['Price vs. Levels'] == 'Near Support'].sort_values(by='Final Quant Score', ascending=False).head(10)

        # --- ⭐️ NEW: Top Quant + High R/R Table ---
        # Filter the Top 20 list for stocks with R/R > 1
        top_quant_high_rr = top_20_quant[top_20_quant['Risk/Reward Ratio'] > 1].sort_values(by='Risk/Reward Ratio', ascending=False)
        # --- ⭐️ END NEW ---


        # --- Save to Excel ---
        excel_file_path = CONFIG['EXCEL_FILE_PATH']
        try:
            with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:
                results_df.to_excel(writer, sheet_name='All Results', index=True)

                # --- ⭐️ NEW: Save new table to Excel ---
                top_10_market_cap.to_excel(writer, sheet_name='Top 10 by Market Cap (SPUS)', index=True)
                # --- ⭐️ END NEW ---

                top_20_quant.to_excel(writer, sheet_name='Top 20 Final Quant Score', index=True)

                # --- ⭐️ NEW: Save new table to Excel ---
                top_quant_high_rr.to_excel(writer, sheet_name='Top Quant & High R-R', index=True)
                # --- ⭐️ END NEW ---

                top_10_undervalued.to_excel(writer, sheet_name='Top 10 Undervalued (Graham)', index=True)
                new_crossovers.to_excel(writer, sheet_name='New Bullish Crossovers (MACD)', index=True)
                near_support.to_excel(writer, sheet_name='Stocks Currently Near Support', index=True)

                # Auto-adjust column widths
                for sheet_name in writer.sheets:
                    worksheet = writer.sheets[sheet_name]
                    for col in worksheet.columns:
                        max_length = 0
                        column = col[0].column_letter # Get the column name
                        if not col:
                            continue

                        # Include header length
                        try:
                           header_length = len(str(col[0].value))
                           if header_length > max_length:
                                max_length = header_length
                        except:
                            pass # Handle errors

                        for cell in col:
                            # Skip the header cell
                            if cell.row == 1:
                                continue

                            if cell.value:
                                try:
                                    # Adjust for string length
                                    if len(str(cell.value)) > max_length:
                                        max_length = len(str(cell.value))
                                except:
                                    pass # Handle errors

                        # Add a little padding and limit max width
                        adjusted_width = min((max_length + 2) * 1.2, 50)
                        worksheet.column_dimensions[column].width = adjusted_width

                        # Bold the header row
                        if worksheet[f"{column}1"].font:
                           worksheet[f"{column}1"].font = Font(bold=True)

            logging.info(f"Successfully saved all analysis results to '{excel_file_path}'")
        except Exception as e:
            logging.error(f"Failed to save Excel file: {e}")

        # --- Generate PDF Report (Assuming imports are present) ---
        try:
            from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
            from reportlab.lib.styles import getSampleStyleSheet
            from reportlab.lib import colors
            from reportlab.lib.pagesizes import landscape, letter
            from reportlab.lib.units import inch

            # Create a timestamped PDF file path
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            base_pdf_path = os.path.splitext(excel_file_path)[0] # Use Excel path base
            pdf_file_path = f"{base_pdf_path}_{timestamp}.pdf"

            doc = SimpleDocTemplate(pdf_file_path, pagesize=landscape(letter))
            elements = []

            # --- THIS IS THE CORRECTED LINE ---
            styles = getSampleStyleSheet()
            # --- END CORRECTION ---

            # Title
            elements.append(Paragraph(f"SPUS Analysis Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}", styles['h1']))
            elements.append(Spacer(1, 0.25*inch))

            # --- Define Table Style ---
            table_style = TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.green),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('TEXTCOLOR', (0, 1), (-1, -1), colors.black),
                ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
                ('FONTSIZE', (0, 1), (-1, -1), 9),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                 ('ALTERNATINGBACKGROUND', (0, 1), (-1, -1), [colors.Color(0.9, 0.9, 0.9), colors.Color(0.98, 0.98, 0.98)])
            ])


            # --- ⭐️ PDF HELPER FUNCTION (MODIFIED FOR SHORT HEADERS) ---
            def create_pdf_table(title, df):
                if df.empty:
                    return [Paragraph(f"No data for: {title}", styles['h2']), Spacer(1, 0.1*inch)]

                # Reset index to include 'Ticker' as a column
                df_reset = df.reset_index()

                # --- ⭐️ CHANGED: Updated PDF table columns & headers ---

                # --- ⭐️ NEW TABLE: Top 10 by Market Cap ---
                if title == 'Top 10 by Market Cap (from SPUS)':
                     df_pdf = df_reset[['Ticker', 'Market Cap', 'Sector', 'Last Price', 'Final Quant Score', 'Risk/Reward Ratio', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)']]
                     df_pdf.columns = ['Ticker', 'Mkt Cap', 'Sector', 'Price', 'Score', 'R/R', 'Stop Loss', 'Fib Target', 'Headline', 'Div %']
                # --- ⭐️ END NEW TABLE ---

                elif title == 'Top 20 by Final Quant Score':
                    # --- ⭐️ MODIFIED BY USER REQUEST ---
                    df_pdf = df_reset[['Ticker', 'Final Quant Score', 'Sector', 'Last Price', 'Risk/Reward Ratio', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)', 'Next Earnings Date', 'Shares to Buy ($50 Risk)']]
                    df_pdf.columns = ['Ticker', 'Score', 'Sector', 'Price', 'R/R', 'Stop Loss', 'Fib Target', 'Headline', 'Div %', 'Earnings', 'Shares']

                elif title == 'Top Quant & High R/R (Ratio > 1)':
                     # --- ⭐️ MODIFIED BY USER REQUEST ---
                     df_pdf = df_reset[['Ticker', 'Final Quant Score', 'Risk/Reward Ratio', 'Last Price', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)', 'Shares to Buy ($50 Risk)', 'Next Earnings Date']]
                     df_pdf.columns = ['Ticker', 'Score', 'R/R', 'Price', 'Stop Loss', 'Fib Target', 'Headline', 'Div %', 'Shares', 'Earnings']

                elif title == 'Top 10 Undervalued (Graham)':
                    # --- ⭐️ MODIFIED BY USER REQUEST ---
                    df_pdf = df_reset[['Ticker', 'Final Quant Score', 'Last Price', 'Fair Price (Graham)', 'Risk/Reward Ratio', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)']]
                    df_pdf.columns = ['Ticker', 'Score', 'Price', 'Graham Price', 'R/R', 'Stop Loss', 'Fib Target', 'Headline', 'Div %']

                elif title == 'New Bullish Crossovers (MACD)':
                     # --- ⭐️ MODIFIED BY USER REQUEST ---
                     df_pdf = df_reset[['Ticker', 'Final Quant Score', 'MACD_Signal', 'Last Price', 'Trend (50/200 Day MA)', 'Risk/Reward Ratio', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)']]
                     df_pdf.columns = ['Ticker', 'Score', 'MACD', 'Price', 'Trend', 'R/R', 'Stop Loss', 'Fib Target', 'Headline', 'Div %']

                elif title == 'Stocks Currently Near Support':
                     # --- ⭐️ MODIFIED BY USER REQUEST ---
                     df_pdf = df_reset[['Ticker', 'Final Quant Score', 'Price vs. Levels', 'Last Price', 'Risk % (to Support)', 'Risk/Reward Ratio', 'Cut Loss Level (Support)', 'Fib 161.8% Target', 'Latest Headline', 'Dividend Yield (%)']]
                     df_pdf.columns = ['Ticker', 'Score', 'vs. Levels', 'Price', 'Risk %', 'R/R', 'Stop Loss', 'Fib Target', 'Headline', 'Div %']
                else:
                    df_pdf = df_reset # Fallback
                # --- ⭐️ END CHANGE ---

                data = [df_pdf.columns.tolist()] + df_pdf.values.tolist()

                # Format numbers in data for PDF
                formatted_data = [data[0]] # Add header
                for row in data[1:]:
                    new_row = []
                    for item in row:
                        if isinstance(item, (float, int)):
                            new_row.append(f"{item:.2f}")
                        else:
                            new_row.append(str(item))
                    formatted_data.append(new_row)

                table = Table(formatted_data, hAlign='LEFT')
                table.setStyle(table_style)

                elements = [Paragraph(title, styles['h2']), Spacer(1, 0.1*inch), table, Spacer(1, 0.25*inch)]
                return elements

            # --- Add tables to PDF ---
            # --- ⭐️ NEW: Add new table to PDF (at the beginning) ---
            elements.extend(create_pdf_table("Top 10 by Market Cap (from SPUS)", top_10_market_cap))
            # --- ⭐️ END NEW ---

            elements.extend(create_pdf_table("Top 20 by Final Quant Score", top_20_quant))

            # --- ⭐️ NEW: Add new table to PDF ---
            elements.extend(create_pdf_table("Top Quant & High R/R (Ratio > 1)", top_quant_high_rr))
            # --- ⭐️ END NEW ---

            elements.extend(create_pdf_table("Top 10 Undervalued (Graham)", top_10_undervalued))
            elements.extend(create_pdf_table("New Bullish Crossovers (MACD)", new_crossovers))
            elements.extend(create_pdf_table("Stocks Currently Near Support", near_support))


            # Build the PDF
            doc.build(elements)
            logging.info(f"PDF report saved to {pdf_file_path}")

        except ImportError:
            logging.error("reportlab library not found. PDF report was not generated.")
            logging.error("Please install it using: pip install reportlab")
        except Exception as e:
            logging.error(f"Failed to generate PDF report: {e}")

    logging.info("\n--- Analysis Complete ---")